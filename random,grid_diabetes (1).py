# -*- coding: utf-8 -*-
"""RANDOM,GRID DIABETES

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YlJinhFCYesAKsqnfVCSyEM2k0HqxlPa
"""

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import string
import nltk
import re
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score,precision_score,recall_score
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV,RandomizedSearchCV

import warnings
warnings.filterwarnings("ignore")

from google.colab import drive
drive.mount("/content/drive")

b=pd.read_csv("/content/drive/MyDrive/diabetes.zip")

print(b.head())
print(b.describe)
print(b.info())

print(b.isnull().sum())
print(b.duplicated)

print(b.drop_duplicates(inplace=True))
print(b.duplicated().sum())
print(b["Gender"].value_counts())
print(b.dropna())

print(b.shape)
print(b["Blood Pressure"].value_counts())

b.head()

""" COUNTPLOT"""

# prompt: build plot

# Assuming 'b' is your DataFrame from the previous code
plt.figure(figsize=(10, 6))
sns.countplot(x='Gender', data=b)
plt.title('Gender Distribution')

plt.show()

"""HISTPLOT"""

plt.figure(figsize=(7,8))
sns.histplot(x='Smoking',data=b)

"""Boxplot"""

plt.figure(figsize=(10,7))
sns.boxplot(x="Gender",y="Age",data=b)
plt.title("Gender")

"""PIECHAT"""

plt.figure(figsize=(10,8))
gender=b["Gender"].value_counts()
plt.pie(gender,labels=["Male","Female"],autopct="%1.1f%%")

"""**BARPLOT**"""

# prompt: build barplot

# Barplot
plt.figure(figsize=(10, 6))
sns.barplot(x='Gender', y='Age', data=b, estimator=np.mean)  # Use 'estimator' to specify aggregation
plt.title('Average Age by Gender')
plt.xlabel('Gender')
plt.ylabel('Average Age')
plt.show()

b.hist()

# prompt: build pairplot code

# Pairplot
plt.figure(figsize=(10, 8))
sns.pairplot(b, hue='Gender')  # Use 'hue' to color points by gender
plt.show()

# Create the scatter plot using your provided data
plt.figure(figsize=(10, 6))  # Adjust figure size if needed
x = b['Age']  # Replace 'Age' with your desired x-axis column
y = b['Blood Pressure'] #Replace 'Blood Pressure' with your desired y-axis column
colors = b['BMI']  # Replace with your color column
#sizes = 10 * b['Glucose']  # Replace with your size column (scaled appropriately)


plt.scatter(x, y, c=colors,  alpha=0.5, cmap='nipy_spectral')
plt.colorbar(label='BMI') # Set colorbar label

plt.xlabel('Age') # Set x-axis label
plt.ylabel('Blood Pressure') # Set y-axis label
plt.title('Scatter Plot of Age vs Blood Pressure') # Set plot title
plt.show()

b.head()

"""LABEL ENCODER"""

le=LabelEncoder()
columns=["Gender","Blood Pressure","Family History of Diabetes","Smoking","Diet","Exercise","Diagnosis"]
for col in columns:
  b[col]=le.fit_transform(b[col])

b.head()

y=b["Diagnosis"]
x=b.drop(columns=["Diagnosis"])

print(x.shape)
print(y.shape)

x_train,x_temp,y_train,y_temp=train_test_split(x,y,test_size=0.3,random_state=42)
x_test,x_cv,y_test,y_cv=train_test_split(x_train,y_train,test_size=0.4,random_state=42)

print(x_train.shape)
print(x_test.shape)
print(x_cv.shape)
print(x_temp.shape)
print(y_test.shape)
print(y_train.shape)
print(y_cv.shape)
print(y_temp.shape)

"""KNNCLASSIFIER"""

knn=KNeighborsClassifier(n_neighbors=3)
knn.fit(x_train,y_train)
y_test_pred=knn.predict(x_test)
y_cv_pred=knn.predict(x_cv)

accu=accuracy_score(y_test,y_test_pred)
cv_accu=accuracy_score(y_cv,y_cv_pred)
cm=confusion_matrix(y_cv,y_cv_pred)

test_prec=precision_score(y_test,y_test_pred,average="macro")
cv_prec=precision_score(y_cv,y_cv_pred,average="macro")

test_rec=recall_score(y_test,y_test_pred,average="macro")
cv_rec=recall_score(y_cv,y_cv_pred,average="macro")

print(accu)
print(cv_accu)
print(test_prec)
print(cv_prec)
print(test_rec)
print(cv_rec)
print(cm)

"""CONFUSION MATRIX HEAT MAP"""

sns.heatmap(cm,fmt="d",annot=True)

"""HYPERPARAMETER TUNING FOR KNN"""

import math
k=list(range(1,50,2))
train_accuracy=[]
cv_accuracy=[]
for i in k:
  knn=KNeighborsClassifier(n_neighbors=i)
  knn.fit(x_train,y_train)
  y_train_pred=knn.predict(x_train)
  y_cv_pred=knn.predict(x_cv)
  train_accuracy.append(accuracy_score(y_train,y_train_pred))
  cv_accuracy.append(accuracy_score(y_cv,y_cv_pred))
optimal_k=k[cv_accuracy.index(max(cv_accuracy))]
k_log=[math.log(x)for x in k]
print(optimal_k)

"""PLOT GRAPH"""

# Plot accuracy vs log(k)
plt.figure(figsize=(8, 6))
plt.plot(k_log, train_accuracy, label='Train Accuracy')
plt.plot(k_log, cv_accuracy, label='CV Accuracy')
plt.title('Accuracy vs Hyperparameter (k)')
plt.xlabel('log(k)')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""OPTIMAL_K KNN"""

knn=KNeighborsClassifier(n_neighbors=optimal_k)
knn.fit(x_train,y_train)
y_train_pred_h=knn.predict(x_train)
y_test_pred_h=knn.predict(x_test)
y_cv_pred_h=knn.predict(x_cv)

accu_h=accuracy_score(y_train,y_train_pred_h)
test_accu_h=accuracy_score(y_test,y_test_pred_h)
cv_accu_h=accuracy_score(y_cv,y_cv_pred_h)
cm=confusion_matrix(y_test,y_test_pred_h)


test_prec_h=precision_score(y_test,y_test_pred_h,average="macro")
train_prec_h=precision_score(y_train,y_train_pred_h,average="macro")
cv_prec_h=precision_score(y_cv,y_cv_pred_h,average="macro")

test_recall_h=recall_score(y_test,y_test_pred_h,average="macro")
train_recall_h=recall_score(y_train,y_train_pred_h,average="macro")
cv_recall_h=recall_score(y_cv,y_cv_pred_h,average="macro")



print(accu_h)
print(test_accu_h)
print(cv_accu_h)
print(test_prec_h)
print(train_prec_h)
print(cv_prec_h)
print(test_recall_h)
print(train_recall_h)
print(cv_recall_h)

print(cm)

"""confusion matrix for hyperparameter tuning"""

sns.heatmap(cm,annot=True,fmt="d")

col=['Model' , 'Train-Accuracy', 'Test-Accuracy', 'cv_accuracy' ,'train_precision','test_precision','cv_precision','train_recall','test_recall','cv_recall']
knn= pd.DataFrame([['KNN',accu_h,test_accu_h,cv_accu_h,train_prec_h,test_prec_h,cv_prec_h,train_recall_h,test_recall_h,cv_recall_h]],columns=col)
#results.loc[1] = new

knn

"""NAIVE BAYES CODE"""



mnb=MultinomialNB()
mnb.fit(x_train,y_train)
y_test_pred_naive=mnb.predict(x_test)
y_cv_pred_naive=mnb.predict(x_cv)

test_accu=accuracy_score(y_test,y_test_pred_naive)
cv_accu=accuracy_score(y_cv,y_cv_pred_naive)
test_precision = precision_score(y_test, y_test_pred_naive, average='micro')
cv_precision = precision_score(y_cv, y_cv_pred_naive, average='micro')

test_recall = recall_score(y_test, y_test_pred_naive, average='micro')
cv_recall = recall_score(y_cv, y_cv_pred_naive, average='micro')

cm=confusion_matrix(y_test,y_test_pred_naive)

print(test_accu)
print(cv_accu)
print(test_precision)
print(cv_precision)
print(test_recall)
print(cv_recall)
print(cm)

"""HYPERPARAMETER TUNING FOR NAIVE BAYES"""

alpha = [10000,1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001]

train_auc = []
cv_auc = []

for i in alpha:
    clf = MultinomialNB(alpha = i)
    clf.fit(x_train, y_train)

    y_train_pred_naives= clf.predict(x_train)
    y_cv_pred_naives = clf.predict(x_cv)


    train_auc.append(accuracy_score(y_train, y_train_pred_naives))
    cv_auc.append(accuracy_score(y_cv, y_cv_pred_naives))

optimal_alpha= alpha[cv_auc.index(max(cv_auc))]
alpha=[math.log(x) for x in alpha]

print(optimal_alpha)

"""plot graph for hyperparameter tuning naive bayes"""

# Plot accuracy vs log(k)
plt.figure(figsize=(8, 6))
plt.plot(alpha, train_auc, label='Train Accuracy')
plt.plot(alpha, cv_auc, label='CV Accuracy')
plt.title('Accuracy vs Hyperparameter (alpha)')
plt.xlabel('log(alpha)')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""OPTIMAL_ALPHA FOR NAIVE BAYES"""

clf = MultinomialNB(alpha = optimal_alpha )
clf.fit(x_train, y_train)


# Predict on the test set and cross-validation set
y_train_pred_naive=clf.predict(x_train)
y_test_pred_naive= clf.predict(x_test)
y_cv_pred_naive = clf.predict(x_cv)

# Evaluate the model
train_accuracy_nb= accuracy_score(y_train,y_train_pred_naive)
test_accuracy_nb = accuracy_score(y_test, y_test_pred_naive)
cv_accuracy_nb = accuracy_score(y_cv, y_cv_pred_naive)

train_precision_nb= precision_score(y_train,y_train_pred_naive, average='micro')
test_precision_nb = precision_score(y_test, y_test_pred_naive, average='micro')
cv_precision_nb = precision_score(y_cv, y_cv_pred_naive, average='micro')

train_recall_nb= recall_score(y_train,y_train_pred_naive, average='micro')
test_recall_nb = recall_score(y_test, y_test_pred_naive, average='micro')
cv_recall_nb = recall_score(y_cv, y_cv_pred_naive, average='micro')



cm_test = confusion_matrix(y_test, y_test_pred_naive)

print(train_accuracy_nb)
print(test_accuracy_nb)
print(cv_accuracy_nb)
print(train_precision_nb)
print(test_precision_nb)
print(cv_precision_nb)
print(train_recall_nb)
print(test_recall_nb)
print(cv_recall_nb)

print(cm_test)

"""CONFUSION MATRIX FOR HYPERPARAMETER TUNING FOR NAIVE BAYES"""

#Compute the confusion matrix for the test set

cm_test = confusion_matrix(y_test, y_test_pred_naive)

# Plot confusion matrix for the test set
plt.figure(figsize=(8, 6))
sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Test Set")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

col=['Model' , 'Train-Accuracy', 'Test-Accuracy', 'cv_accuracy' ,'train_precision','test_precision','cv_precision','train_recall','test_recall','cv_recall']
naivebayes= pd.DataFrame([['naivebayes',train_accuracy_nb,test_accuracy_nb,cv_accuracy_nb,train_precision_nb,test_precision_nb,cv_precision_nb,train_recall_nb,test_recall_nb,cv_recall_nb]],columns=col)
#results.loc[1] = new

"""LOGISTIC REGRESSION OF L1"""

from sklearn.linear_model import LogisticRegression
l1_model=LogisticRegression(penalty="l1",solver="liblinear",random_state=42)
l1_model.fit(x_train,y_train)
y_test_pred_log=l1_model.predict(x_test)
y_cv_pred_log=l1_model.predict(x_cv)

test_accu=accuracy_score(y_test,y_test_pred_log)
cv_accu=accuracy_score(y_cv,y_cv_pred_log)
test_precision = precision_score(y_test, y_test_pred_log, average='micro')
cv_precision = precision_score(y_cv, y_cv_pred_log, average='micro')

test_recall = recall_score(y_test, y_test_pred_log, average='micro')
cv_recall = recall_score(y_cv, y_cv_pred_log, average='micro')

cm=confusion_matrix(y_test,y_test_pred_log)

print(test_accu)
print(cv_accu)
print(test_precision)
print(cv_precision)
print(test_recall)
print(cv_recall)
print(cm)

c= [10000,1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001]

train_auc = []
cv_auc = []

for i in c:
    l1_model= LogisticRegression(penalty="l1",C=i,solver="liblinear")
    l1_model.fit(x_train, y_train)

    y_train_pred= l1_model.predict(x_train)
    y_cv_pred = l1_model.predict(x_cv)


    train_auc.append(accuracy_score(y_train, y_train_pred))
    cv_auc.append(accuracy_score(y_cv, y_cv_pred))

optimal_c= c[cv_auc.index(max(cv_auc))]
c=[math.log(x) for x in c]

print(optimal_c)

# Plot accuracy vs log(k)
plt.figure(figsize=(8, 6))
plt.plot(c, train_auc, label='Train Accuracy')
plt.plot(c, cv_auc, label='CV Accuracy')
plt.title('Accuracy vs Hyperparameter (c)')
plt.xlabel('log(c)')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

l1_model=LogisticRegression(penalty="l1",C=optimal_c,solver="liblinear")
l1_model.fit(x_train, y_train)


# Predict on the test set and cross-validation set
y_train_pred_log_h=l1_model.predict(x_train)
y_test_pred_log_h= l1_model.predict(x_test)
y_cv_pred_log_h = l1_model.predict(x_cv)

train_accu=accuracy_score(y_train,y_train_pred_log_h)
test_accu=accuracy_score(y_test,y_test_pred_log_h)
cv_accu=accuracy_score(y_cv,y_cv_pred_log_h)

train_precision=precision_score(y_train,y_train_pred_log_h)
test_precision = precision_score(y_test, y_test_pred_log_h, average='micro')
cv_precision = precision_score(y_cv, y_cv_pred_log_h, average='micro')

train_recall=recall_score(y_train,y_train_pred_log_h)
test_recall = recall_score(y_test, y_test_pred_log_h, average='micro')
cv_recall = recall_score(y_cv, y_cv_pred_log_h, average='micro')

cm=confusion_matrix(y_test,y_test_pred_log_h)

print(train_accu)
print(test_accu)
print(cv_accu)
print(train_precision)
print(test_precision)
print(cv_precision)
print(train_recall)
print(test_recall)
print(cv_recall)
print(cm)

col=['Model' , 'Train-Accuracy', 'Test-Accuracy', 'cv_accuracy' ,'train_precision','test_precision','cv_precision','train_recall','test_recall','cv_recall']
L1= pd.DataFrame([['L1',train_accu,test_accu,cv_accu,train_precision,test_precision,cv_precision,train_recall,test_recall,cv_recall]],columns=col)
#results.loc[1] = new

"""LOGISTIC REGRESSION FOR L2"""

from sklearn.linear_model import LogisticRegression
l2_model=LogisticRegression(penalty="l2",solver="liblinear",random_state=42)
l2_model.fit(x_train,y_train)
y_test_pred_log2=l2_model.predict(x_test)
y_cv_pred_log2=l2_model.predict(x_cv)

test_accu=accuracy_score(y_test,y_test_pred_log2)
cv_accu=accuracy_score(y_cv,y_cv_pred_log2)
test_precision = precision_score(y_test, y_test_pred_log2, average='micro')
cv_precision = precision_score(y_cv, y_cv_pred_log2, average='micro')

test_recall = recall_score(y_test, y_test_pred_log2, average='micro')
cv_recall = recall_score(y_cv, y_cv_pred_log2, average='micro')

cm=confusion_matrix(y_test,y_test_pred_log2)

print(test_accu)
print(cv_accu)
print(test_precision)
print(cv_precision)
print(test_recall)
print(cv_recall)
print(cm)

"""HYPERPARAMETER TUNING FOR L2"""

c= [10000,1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001]

train_auc = []
cv_auc = []

for i in c:
    l2_model= LogisticRegression(penalty="l2",C=i,solver="liblinear")
    l2_model.fit(x_train, y_train)

    y_train_pred_log2 = l2_model.predict(x_train)
    y_cv_pred_log2= l2_model.predict(x_cv)


    train_auc.append(accuracy_score(y_train, y_train_pred_log2))
    cv_auc.append(accuracy_score(y_cv, y_cv_pred_log2))

optimal_c= c[cv_auc.index(max(cv_auc))]
c=[math.log(x) for x in c]
print(optimal_c)

# Plot accuracy vs log(k)
plt.figure(figsize=(8, 6))
plt.plot(c, train_auc, label='Train Accuracy')
plt.plot(c, cv_auc, label='CV Accuracy')
plt.title('Accuracy vs Hyperparameter (c)')
plt.xlabel('log(c)')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

l2_model=LogisticRegression(penalty="l2",C=optimal_c,solver="liblinear")
l2_model.fit(x_train, y_train)


# Predict on the test set and cross-validation set
y_train_pred_log2=l2_model.predict(x_train)
y_test_pred_log2 = l2_model.predict(x_test)
y_cv_pred_log2= l2_model.predict(x_cv)

train_accu=accuracy_score(y_train,y_train_pred_log2)
test_accu=accuracy_score(y_test,y_test_pred_log_h)
cv_accu=accuracy_score(y_cv,y_cv_pred_log_h)

train_precision=precision_score(y_train,y_train_pred_log2)
test_precision = precision_score(y_test, y_test_pred_log_h, average='micro')
cv_precision = precision_score(y_cv, y_cv_pred_log_h, average='micro')

train_recall=recall_score(y_train,y_train_pred_log2)
test_recall = recall_score(y_test, y_test_pred_log_h, average='micro')
cv_recall = recall_score(y_cv, y_cv_pred_log_h, average='micro')

cm=confusion_matrix(y_test,y_test_pred_log_h)

print(train_accu)
print(test_accu)
print(cv_accu)
print(train_precision)
print(test_precision)
print(cv_precision)
print(train_recall)
print(test_recall)
print(cv_recall)
print(cm)

sns.heatmap(cm,annot=True,fmt="d")

col=['Model' , 'Train-Accuracy', 'Test-Accuracy', 'cv_accuracy' ,'train_precision','test_precision','cv_precision','train_recall','test_recall','cv_recall']
L2= pd.DataFrame([['L2',train_accu,test_accu,cv_accu,train_precision,test_precision,cv_precision,train_recall,test_recall,cv_recall]],columns=col)
#results.loc[1] = new

from sklearn.linear_model import LogisticRegression
elastic_net_model=LogisticRegression(penalty="elasticnet",solver="saga",random_state=42,l1_ratio=0.5)
elastic_net_model.fit(x_train,y_train)
y_test_pred_ela=elastic_net_model.predict(x_test)
y_cv_pred_ela=elastic_net_model.predict(x_cv)

test_accu=accuracy_score(y_test,y_test_pred_ela)
cv_accu=accuracy_score(y_cv,y_cv_pred_ela)
test_precision = precision_score(y_test, y_test_pred_ela, average='micro')
cv_precision = precision_score(y_cv, y_cv_pred_ela, average='micro')

test_recall = recall_score(y_test, y_test_pred_ela, average='micro')
cv_recall = recall_score(y_cv, y_cv_pred_ela, average='micro')

cm=confusion_matrix(y_test,y_test_pred_ela)

print(test_accu)
print(cv_accu)
print(test_precision)
print(cv_precision)
print(test_recall)
print(cv_recall)
print(cm)

sns.heatmap(cm,annot=True,fmt="d")

c= [10000,1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001]

train_auc = []
cv_auc = []

for i in c:
    elastic_net_model= LogisticRegression(penalty="elasticnet",C=i,solver="saga",l1_ratio=0.5)
    elastic_net_model.fit(x_train, y_train)

    y_train_pred_ela = elastic_net_model.predict(x_train)
    y_cv_pred_ela= elastic_net_model.predict(x_cv)


    train_auc.append(accuracy_score(y_train, y_train_pred_ela))
    cv_auc.append(accuracy_score(y_cv, y_cv_pred_ela))

optimal_c= c[cv_auc.index(max(cv_auc))]
c=[math.log(x) for x in c]
print(optimal_c)

# Plot accuracy vs log(k)
plt.figure(figsize=(8, 6))
plt.plot(c, train_auc, label='Train Accuracy')
plt.plot(c, cv_auc, label='CV Accuracy')
plt.title('Accuracy vs Hyperparameter (c)')
plt.xlabel('log(c)')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

elastic_net_model=LogisticRegression(penalty="elasticnet",C=optimal_c,solver="saga",l1_ratio=0.5)
elastic_net_model.fit(x_train, y_train)


# Predict on the test set and cross-validation set
y_train_pred_ela=elastic_net_model.predict(x_train)
y_test_pred_ela= elastic_net_model.predict(x_test)
y_cv_pred_ela = elastic_net_model.predict(x_cv)

train_accu=accuracy_score(y_train,y_train_pred_ela)
test_accu=accuracy_score(y_test,y_test_pred_ela)
cv_accu=accuracy_score(y_cv,y_cv_pred_ela)

train_precision=precision_score(y_train,y_train_pred_ela)
test_precision = precision_score(y_test, y_test_pred_ela, average='micro')
cv_precision = precision_score(y_cv, y_cv_pred_ela, average='micro')

train_recall=recall_score(y_train,y_train_pred_ela)
test_recall = recall_score(y_test, y_test_pred_ela, average='micro')
cv_recall = recall_score(y_cv, y_cv_pred_ela, average='micro')

cm=confusion_matrix(y_test,y_test_pred_ela)

print(train_accu)
print(test_accu)
print(cv_accu)
print(train_precision)
print(test_precision)
print(cv_precision)
print(train_recall)
print(test_recall)
print(cv_recall)
print(cm)

sns.heatmap(cm,annot=True,fmt="d")

col=['Model' , 'Train-Accuracy', 'Test-Accuracy', 'cv_accuracy' ,'train_precision','test_precision','cv_precision','train_recall','test_recall','cv_recall']
ElasticNet= pd.DataFrame([['ElasticNet',train_accu,test_accu,cv_accu,train_precision,test_precision,cv_precision,train_recall,test_recall,cv_recall]],columns=col)
#results.loc[1] = new

"""LINEARSVC ALGORITHM"""

from sklearn.linear_model import SGDClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import roc_auc_score
from sklearn.svm import LinearSVC



log = LinearSVC()
log.fit(x_train, y_train)

# Predict on the test set and cross-validation set
y_test_pred = log.predict(x_test)
y_cv_pred = log.predict(x_cv)

# Evaluate the model
test_accuracy = accuracy_score(y_test, y_test_pred)
cv_accuracy = accuracy_score(y_cv, y_cv_pred)

test_precision = precision_score(y_test, y_test_pred, average='micro')
cv_precision = precision_score(y_cv, y_cv_pred, average='micro')

test_recall = recall_score(y_test, y_test_pred, average='micro')
cv_recall = recall_score(y_cv, y_cv_pred, average='micro')

cm_s=confusion_matrix(y_test,y_test_pred)

print(test_accuracy)
print(cv_accuracy)
print(test_precision)
print(cv_precision)
print(test_recall)
print(cv_recall)
print(cm_s)

# Plot confusion matrix for the test set
plt.figure(figsize=(8, 6))
sns.heatmap(cm_s, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Test Set")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

c= [10000,1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001]

train_auc = []
cv_auc = []

for i in c:

    clf = LinearSVC(C=i)
    clf.fit(x_train, y_train)

    y_train_pred_svc = clf.predict(x_train)
    y_cv_pred_svc = clf.predict(x_cv)


    train_auc.append(accuracy_score(y_train, y_train_pred_svc))
    cv_auc.append(accuracy_score(y_cv, y_cv_pred_svc))

optimal_c= c[cv_auc.index(max(cv_auc))]
c=[math.log(x) for x in c]

print(optimal_c)

# Plot accuracy vs log(k)
plt.figure(figsize=(8, 6))
plt.plot(c, train_auc, label='Train Accuracy')
plt.plot(c, cv_auc, label='CV Accuracy')
plt.title('Accuracy vs Hyperparameter (c)')
plt.xlabel('log(c)')
plt.ylabel('Accuracy')
plt.legend()
plt.show()



clf = LinearSVC(C=optimal_c)
clf.fit(x_train, y_train)


# Predict on the test set and cross-validation set
y_train_pred=clf.predict(x_train)
y_test_pred = clf.predict(x_test)
y_cv_pred = clf.predict(x_cv)

# Evaluate the model
train_accuracy_log= accuracy_score(y_train,y_train_pred)
test_accuracy_log = accuracy_score(y_test, y_test_pred)
cv_accuracy_log = accuracy_score(y_cv, y_cv_pred)

train_precision_log= precision_score(y_train,y_train_pred, average='micro')
test_precision_log = precision_score(y_test, y_test_pred, average='micro')
cv_precision_log = precision_score(y_cv, y_cv_pred, average='micro')

train_recall_log= recall_score(y_train,y_train_pred, average='micro')
test_recall_log = recall_score(y_test, y_test_pred, average='micro')
cv_recall_log = recall_score(y_cv, y_cv_pred, average='micro')



cm=confusion_matrix(y_test,y_test_pred)

print(train_accuracy_log)
print(test_accuracy_log)
print(cv_accuracy_log)
print(train_precision_log)
print(test_precision_log)
print(cv_precision_log)
print(train_recall_log)
print(test_recall_log)
print(cv_recall_log)
print(cm)

sns.heatmap(cm,annot=True,fmt="d")

col=['Model' , 'Train-Accuracy', 'Test-Accuracy', 'cv_accuracy' ,'train_precision','test_precision','cv_precision','train_recall','test_recall','cv_recall']
LINEARSVM= pd.DataFrame([['LINEARSVM',train_accuracy_log,test_accuracy_log,cv_accuracy_log,train_precision_log,test_precision_log,cv_precision_log,train_recall_log,test_recall_log,cv_recall_log]],columns=col)
#results.loc[1] = new

"""RBFM ALGORITHM CODE"""



from sklearn.svm import SVC

rbf_svm = SVC(kernel='rbf')
log.fit(x_train, y_train)

# Predict on the test set and cross-validation set
y_test_pred_rbfm = log.predict(x_test)
y_cv_pred_rbfm = log.predict(x_cv)

# Evaluate the model
test_accuracy = accuracy_score(y_test, y_test_pred_rbfm)
cv_accuracy = accuracy_score(y_cv, y_cv_pred_rbfm)

test_precision = precision_score(y_test, y_test_pred_rbfm, average='micro')
cv_precision = precision_score(y_cv, y_cv_pred_rbfm, average='micro')

test_recall = recall_score(y_test, y_test_pred_rbfm, average='micro')
cv_recall = recall_score(y_cv, y_cv_pred_rbfm, average='micro')


cm_rbfm=confusion_matrix(y_test,y_test_pred_rbfm)

print(test_accuracy)
print(cv_accuracy)
print(test_precision)
print(cv_precision)
print(test_recall)
print(cv_recall)
print(cm_rbfm)

# Plot confusion matrix for the test set
plt.figure(figsize=(8, 6))
sns.heatmap(cm_rbfm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Test Set")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

c= [10000,1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001]

train_auc = []
cv_auc = []

for i in c:

    clf = SVC(C=i,kernel='rbf')
    clf.fit(x_train, y_train)

    y_train_pred_rbfm = clf.predict(x_train)
    y_cv_pred_rbfm = clf.predict(x_cv)


    train_auc.append(accuracy_score(y_train, y_train_pred))
    cv_auc.append(accuracy_score(y_cv, y_cv_pred))

optimal_c= c[cv_auc.index(max(cv_auc))]
c=[math.log(x) for x in c]

print(optimal_c)

# Plot accuracy vs log(k)
plt.figure(figsize=(8, 6))
plt.plot(c, train_auc, label='Train Accuracy')
plt.plot(c, cv_auc, label='CV Accuracy')
plt.title('Accuracy vs Hyperparameter (c)')
plt.xlabel('log(c)')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

clf = SVC(C=optimal_c,kernel='rbf')
clf.fit(x_train, y_train)


# Predict on the test set and cross-validation set
y_train_pred=clf.predict(x_train)
y_test_pred = clf.predict(x_test)
y_cv_pred = clf.predict(x_cv)

# Evaluate the model
train_accuracy_log= accuracy_score(y_train,y_train_pred)
test_accuracy_log = accuracy_score(y_test, y_test_pred)
cv_accuracy_log = accuracy_score(y_cv, y_cv_pred)

train_precision_log= precision_score(y_train,y_train_pred, average='micro')
test_precision_log = precision_score(y_test, y_test_pred, average='micro')
cv_precision_log = precision_score(y_cv, y_cv_pred, average='micro')

train_recall_log= recall_score(y_train,y_train_pred, average='micro')
test_recall_log = recall_score(y_test, y_test_pred, average='micro')
cv_recall_log = recall_score(y_cv, y_cv_pred, average='micro')

cm_rbfm=confusion_matrix(y_test,y_test_pred)

print(train_accuracy_log)
print(test_accuracy_log)
print(cv_accuracy_log)
print(train_precision_log)
print(test_precision_log)
print(cv_precision_log)
print(train_recall_log)
print(test_recall_log)
print(cv_recall_log)

print(cm_rbfm)

sns.heatmap(cm_rbfm,annot=True,fmt="d")

col=['Model' , 'Train-Accuracy', 'Test-Accuracy', 'cv_accuracy' ,'train_precision','test_precision','cv_precision','train_recall','test_recall','cv_recall']
LINEARRBFM= pd.DataFrame([['LINEARRBFM',train_accuracy_log,test_accuracy_log,cv_accuracy_log,train_precision_log,test_precision_log,cv_precision_log,train_recall_log,test_recall_log,cv_recall_log]],columns=col)
#results.loc[1] = new

"""DECISIONTREE CLASSIFIER ALGORITHM(GRID SEARCH CV)

"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import GridSearchCV


dept = [1, 5, 10, 50, 100, 500, 1000]
min_samples =  [5, 10, 100, 500]


param_grid={'min_samples_split':min_samples , 'max_depth':dept}
clf = DecisionTreeClassifier()

model = GridSearchCV(clf,param_grid,scoring='roc_auc',n_jobs=-1,cv=3)
model.fit(x_train, y_train)

print("optimal min_samples_split",model.best_estimator_.min_samples_split)
print("optimal max_depth",model.best_estimator_.max_depth)

"""HYPERPARAMETER TUNING FOR DECISIONTREECLASSIFIER"""

clf = DecisionTreeClassifier(max_depth = 1,min_samples_split = 5)
clf.fit(x_train,y_train)
# Predict on the test set and cross-validation set
y_train_pred=clf.predict(x_train)
y_test_pred = clf.predict(x_test)
y_cv_pred = clf.predict(x_cv)

# Evaluate the model
train_accuracy_dt= accuracy_score(y_train,y_train_pred)
test_accuracy_dt = accuracy_score(y_test, y_test_pred)
cv_accuracy_dt = accuracy_score(y_cv, y_cv_pred)

train_precision_dt= precision_score(y_train,y_train_pred, average='micro')
test_precision_dt = precision_score(y_test, y_test_pred, average='micro')
cv_precision_dt = precision_score(y_cv, y_cv_pred, average='micro')

train_recall_dt= recall_score(y_train,y_train_pred, average='micro')
test_recall_dt = recall_score(y_test, y_test_pred, average='micro')
cv_recall_dt = recall_score(y_cv, y_cv_pred, average='micro')


cm_dt=confusion_matrix(y_test,y_test_pred)

print(train_accuracy_dt)
print(test_accuracy_dt)
print(cv_accuracy_dt)
print(train_precision_dt)
print(test_precision_dt)
print(cv_precision_dt)
print(train_recall_dt)
print(test_recall_dt)
print(cv_recall_dt)

print(cm_dt)

col=['Model' , 'Train-Accuracy', 'Test-Accuracy', 'cv_accuracy' ,'train_precision','test_precision','cv_precision','train_recall','test_recall','cv_recall']
DecisionTree1= pd.DataFrame([['DecisionTree(GRIDSEARCHCV)',train_accuracy_dt,test_accuracy_dt,cv_accuracy_dt,train_precision_dt,test_precision_dt,cv_precision_dt,train_recall_dt,test_recall_dt,cv_recall_dt]],columns=col)
#results.loc[1] = new

"""DECISION TREE CLASSIFIER(RANDOMSEARCHCV)"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import RandomizedSearchCV


dept = [1, 5, 10, 50, 100, 500, 1000]
min_samples =  [5, 10, 100, 500]


param_grid={'min_samples_split':min_samples , 'max_depth':dept}
clf = DecisionTreeClassifier()

model = RandomizedSearchCV(clf,param_grid,scoring='roc_auc',n_jobs=-1,cv=3,n_iter=10,random_state=42)
model.fit(x_train, y_train)

print("optimal min_samples_split",model.best_estimator_.min_samples_split)
print("optimal max_depth",model.best_estimator_.max_depth)

clf = DecisionTreeClassifier(max_depth = 1,min_samples_split = 5)
clf.fit(x_train,y_train)
# Predict on the test set and cross-validation set
y_train_pred=clf.predict(x_train)
y_test_pred = clf.predict(x_test)
y_cv_pred = clf.predict(x_cv)

# Evaluate the model
train_accuracy_rd= accuracy_score(y_train,y_train_pred)
test_accuracy_rd = accuracy_score(y_test, y_test_pred)
cv_accuracy_rd= accuracy_score(y_cv, y_cv_pred)

train_precision_rd= precision_score(y_train,y_train_pred, average='micro')
test_precision_rd= precision_score(y_test, y_test_pred, average='micro')
cv_precision_rd = precision_score(y_cv, y_cv_pred, average='micro')

train_recall_rd= recall_score(y_train,y_train_pred, average='micro')
test_recall_rd = recall_score(y_test, y_test_pred, average='micro')
cv_recall_rd = recall_score(y_cv, y_cv_pred, average='micro')


cm_rd=confusion_matrix(y_test,y_test_pred)

print(train_accuracy_rd)
print(test_accuracy_rd)
print(cv_accuracy_rd)
print(train_precision_rd)
print(test_precision_rd)
print(cv_precision_rd)
print(train_recall_rd)
print(test_recall_rd)
print(cv_recall_rd)

print(cm_rd)

col=['Model' , 'Train-Accuracy', 'Test-Accuracy', 'cv_accuracy' ,'train_precision','test_precision','cv_precision','train_recall','test_recall','cv_recall']
DecisionTree= pd.DataFrame([['DECISION(RandomSearchcv)',train_accuracy_rd,test_accuracy_rd,cv_accuracy_rd,train_precision_rd,test_precision_rd,cv_precision_rd,train_recall_rd,test_recall_rd,cv_recall_rd]],columns=col)
#results.loc[1] = new

"""RANDOM FOREST CLASSIFIER ALOGORITHM"""

from sklearn.metrics import accuracy_score


from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import roc_curve

dept = [1, 5, 10, 50, 100, 500, 1000]
n_estimators =  [20, 40, 60, 80, 100, 120]

param_grid={'n_estimators':n_estimators , 'max_depth':dept}
clf = RandomForestClassifier()
model = GridSearchCV(clf,param_grid,scoring='accuracy',n_jobs=-1,cv=3)
model.fit(x_train, y_train)
print("optimal n_estimators",model.best_estimator_.n_estimators)
print("optimal max_depth",model.best_estimator_.max_depth)

optimal_max_depth = model.best_estimator_.max_depth
optimal_n_estimators = model.best_estimator_.n_estimators

clf = RandomForestClassifier(max_depth = optimal_max_depth,n_estimators = optimal_n_estimators)
clf.fit(x_train, y_train)
# Predict on the test set and cross-validation set
y_train_pred=clf.predict(x_train)
y_test_pred = clf.predict(x_test)
y_cv_pred = clf.predict(x_cv)

# Evaluate the model
train_accuracy_rd= accuracy_score(y_train,y_train_pred)
test_accuracy_rd = accuracy_score(y_test, y_test_pred)
cv_accuracy_rd = accuracy_score(y_cv, y_cv_pred)

train_precision_rd= precision_score(y_train,y_train_pred, average='micro')
test_precision_rd = precision_score(y_test, y_test_pred, average='micro')
cv_precision_rd = precision_score(y_cv, y_cv_pred, average='micro')

train_recall_rd= recall_score(y_train,y_train_pred, average='micro')
test_recall_rd = recall_score(y_test, y_test_pred, average='micro')
cv_recall_rd = recall_score(y_cv, y_cv_pred, average='micro')


cm_rd=confusion_matrix(y_test,y_test_pred)

print(train_accuracy_rd)
print(test_accuracy_rd)
print(cv_accuracy_rd)
print(train_precision_rd)
print(test_precision_rd)
print(cv_precision_rd)
print(train_recall_rd)
print(test_recall_rd)
print(cv_recall_rd)

print(cm_rd)

sns.heatmap(cm_rd,annot=True,fmt="d")

col=['Model' , 'Train-Accuracy', 'Test-Accuracy', 'cv_accuracy' ,'train_precision','test_precision','cv_precision','train_recall','test_recall','cv_recall']
RANDOMFOREST1= pd.DataFrame([['RANDOMFOREST(GRID SEARCHCV)',train_accuracy_rd,test_accuracy_rd,cv_accuracy_rd,train_precision_rd,test_precision_rd,cv_precision_rd,train_recall_rd,test_recall_rd,cv_recall_rd]],columns=col)
#results.loc[1] = new

"""RANDOMFOREST (RANDOM SEARCHCV)"""

from sklearn.metrics import roc_auc_score
from sklearn.model_selection import RandomizedSearchCV


dept = [1, 5, 10, 50, 100, 500, 1000]
min_samples =  [5, 10, 100, 500]


param_grid={'min_samples_split':min_samples , 'max_depth':dept}
clf = RandomForestClassifier()

model = RandomizedSearchCV(clf,param_grid,scoring='roc_auc',n_jobs=-1,cv=3,n_iter=10,random_state=42)
model.fit(x_train, y_train)

print("optimal min_samples_split",model.best_estimator_.min_samples_split)
print("optimal max_depth",model.best_estimator_.max_depth)

optimal_max_depth = model.best_estimator_.max_depth
optimal_n_estimators = model.best_estimator_.n_estimators

clf = RandomForestClassifier(max_depth = optimal_max_depth,n_estimators = optimal_n_estimators)
clf.fit(x_train, y_train)
# Predict on the test set and cross-validation set
y_train_pred=clf.predict(x_train)
y_test_pred = clf.predict(x_test)
y_cv_pred = clf.predict(x_cv)

# Evaluate the model
train_accuracy_rd= accuracy_score(y_train,y_train_pred)
test_accuracy_rd = accuracy_score(y_test, y_test_pred)
cv_accuracy_rd = accuracy_score(y_cv, y_cv_pred)

train_precision_rd= precision_score(y_train,y_train_pred, average='micro')
test_precision_rd = precision_score(y_test, y_test_pred, average='micro')
cv_precision_rd = precision_score(y_cv, y_cv_pred, average='micro')

train_recall_rd= recall_score(y_train,y_train_pred, average='micro')
test_recall_rd = recall_score(y_test, y_test_pred, average='micro')
cv_recall_rd = recall_score(y_cv, y_cv_pred, average='micro')


cm_rd=confusion_matrix(y_test,y_test_pred)

print(train_accuracy_rd)
print(test_accuracy_rd)
print(cv_accuracy_rd)
print(train_precision_rd)
print(test_precision_rd)
print(cv_precision_rd)
print(train_recall_rd)
print(test_recall_rd)
print(cv_recall_rd)

print(cm_rd)

col=['Model' , 'Train-Accuracy', 'Test-Accuracy', 'cv_accuracy' ,'train_precision','test_precision','cv_precision','train_recall','test_recall','cv_recall']
RANDOMFOREST= pd.DataFrame([['RANDOMFOREST(RANDOMSEARCHCV)',train_accuracy_rd,test_accuracy_rd,cv_accuracy_rd,train_precision_rd,test_precision_rd,cv_precision_rd,train_recall_rd,test_recall_rd,cv_recall_rd]],columns=col)
#results.loc[1] = new

"""XGBOOST(GRIDSEARCHCV)"""

# #!pip uninstall xgboost -y
# !pip install xgboost==2.1.3
# #!pip uninstall scikit-learn -y
# !pip install scikit-learn==1.2.2

# prompt: xgboost code

import xgboost as xgb
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming X_train, y_train, X_test, y_test, X_cv, y_cv are defined from previous code

# Define parameter grid for XGBoost
param_grid = {
    'n_estimators': [20, 40, 60, 80, 100, 120],
    'max_depth': [1, 5, 10, 50, 100, 500, 1000],
    'learning_rate': [0.01, 0.1, 0.3],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0]
}

# Initialize XGBoost classifier
clf = xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, eval_metric='logloss')

# Perform GridSearchCV
model = GridSearchCV(clf, param_grid, scoring='roc_auc', n_jobs=-1, cv=3)
model.fit(x_train, y_train)

# Print optimal hyperparameters
print("optimal n_estimators:", model.best_estimator_.n_estimators)
print("optimal max_depth:", model.best_estimator_.max_depth)
print("optimal learning_rate:", model.best_estimator_.learning_rate)
print("optimal subsample:", model.best_estimator_.subsample)
print("optimal colsample_bytree:", model.best_estimator_.colsample_bytree)


# Train the model with the best hyperparameters
optimal_n_estimators = model.best_estimator_.n_estimators
optimal_max_depth = model.best_estimator_.max_depth
optimal_learning_rate = model.best_estimator_.learning_rate
optimal_subsample = model.best_estimator_.subsample
optimal_colsample_bytree = model.best_estimator_.colsample_bytree

clf = xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, eval_metric='logloss',
                        n_estimators=optimal_n_estimators, max_depth=optimal_max_depth, learning_rate = optimal_learning_rate,
                        subsample = optimal_subsample, colsample_bytree= optimal_colsample_bytree)

clf.fit(x_train, y_train)

# Predict on the test set and cross-validation set
y_train_pred = clf.predict(x_train)
y_test_pred = clf.predict(x_test)
y_cv_pred = clf.predict(x_cv)

train_accuracy_xgb = accuracy_score(y_train, y_train_pred)
test_accuracy_xgb = accuracy_score(y_test, y_test_pred)
cv_accuracy_xgb = accuracy_score(y_cv, y_cv_pred)

train_precision_xgb = precision_score(y_train, y_train_pred, average='micro')
test_precision_xgb = precision_score(y_test, y_test_pred, average='micro')
cv_precision_xgb = precision_score(y_cv, y_cv_pred, average='micro')

train_recall_xgb = recall_score(y_train, y_train_pred, average='micro')
test_recall_xgb = recall_score(y_test, y_test_pred, average='micro')
cv_recall_xgb = recall_score(y_cv, y_cv_pred, average='micro')

print(train_accuracy_xgb)
print(test_accuracy_xgb)
print(cv_accuracy_xgb)
print(train_precision_xgb)
print(test_precision_xgb)
print(cv_precision_xgb)
print(train_recall_xgb)
print(test_recall_xgb)
print(cv_recall_xgb)

#Confusion Matrix
cm_test = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Test Set")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

col=['Model' , 'Train-Accuracy', 'Test-Accuracy', 'cv_accuracy' ,'train_precision','test_precision','cv_precision','train_recall','test_recall','cv_recall']
XGBOOST1= pd.DataFrame([['XGBOOST(GRIDSEARCHCV)',train_accuracy_xgb,test_accuracy_xgb,cv_accuracy_xgb,train_precision_xgb,test_precision_xgb,cv_precision_xgb,train_recall_xgb,test_recall_xgb,cv_recall_xgb]],columns=col)
#results.loc[1] = new

"""XGBOOST(RANANDOMIZEDSEARCHCV)"""

# prompt: xgboost code

import xgboost as xgb
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming X_train, y_train, X_test, y_test, X_cv, y_cv are defined from previous code

# Define parameter grid for XGBoost
param_grid = {
    'n_estimators': [20, 40, 60, 80, 100, 120],
    'max_depth': [1, 5, 10, 50, 100, 500, 1000],
    'learning_rate': [0.01, 0.1, 0.3],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0]
}

# Initialize XGBoost classifier
clf = xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, eval_metric='logloss')

# Perform RandomizedSearchCV
model = RandomizedSearchCV(clf, param_grid, scoring='roc_auc', n_jobs=-1, cv=3,n_iter=10,random_state=42)
model.fit(x_train, y_train)

# Print optimal hyperparameters
print("optimal n_estimators:", model.best_estimator_.n_estimators)
print("optimal max_depth:", model.best_estimator_.max_depth)
print("optimal learning_rate:", model.best_estimator_.learning_rate)
print("optimal subsample:", model.best_estimator_.subsample)
print("optimal colsample_bytree:", model.best_estimator_.colsample_bytree)


# Train the model with the best hyperparameters
optimal_n_estimators = model.best_estimator_.n_estimators
optimal_max_depth = model.best_estimator_.max_depth
optimal_learning_rate = model.best_estimator_.learning_rate
optimal_subsample = model.best_estimator_.subsample
optimal_colsample_bytree = model.best_estimator_.colsample_bytree

clf = xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, eval_metric='logloss',
                        n_estimators=optimal_n_estimators, max_depth=optimal_max_depth, learning_rate = optimal_learning_rate,
                        subsample = optimal_subsample, colsample_bytree= optimal_colsample_bytree)

clf.fit(x_train, y_train)

# Predict on the test set and cross-validation set
y_train_pred = clf.predict(x_train)
y_test_pred = clf.predict(x_test)
y_cv_pred = clf.predict(x_cv)

train_accuracy_xgb = accuracy_score(y_train, y_train_pred)
test_accuracy_xgb = accuracy_score(y_test, y_test_pred)
cv_accuracy_xgb = accuracy_score(y_cv, y_cv_pred)

train_precision_xgb = precision_score(y_train, y_train_pred, average='micro')
test_precision_xgb = precision_score(y_test, y_test_pred, average='micro')
cv_precision_xgb = precision_score(y_cv, y_cv_pred, average='micro')

train_recall_xgb = recall_score(y_train, y_train_pred, average='micro')
test_recall_xgb = recall_score(y_test, y_test_pred, average='micro')
cv_recall_xgb = recall_score(y_cv, y_cv_pred, average='micro')

print(train_accuracy_xgb)
print(test_accuracy_xgb)
print(cv_accuracy_xgb)
print(train_precision_xgb)
print(test_precision_xgb)
print(cv_precision_xgb)
print(train_recall_xgb)
print(test_recall_xgb)
print(cv_recall_xgb)

col=['Model' , 'Train-Accuracy', 'Test-Accuracy', 'cv_accuracy' ,'train_precision','test_precision','cv_precision','train_recall','test_recall','cv_recall']
XGBOOST= pd.DataFrame([['XGBOOST(Randomsearchcv)',train_accuracy_xgb,test_accuracy_xgb,cv_accuracy_xgb,train_precision_xgb,test_precision_xgb,cv_precision_xgb,train_recall_xgb,test_recall_xgb,cv_recall_xgb]],columns=col)
#results.loc[1] = new

result=pd.DataFrame()
result=pd.concat([result,knn,naivebayes,L1,L2,ElasticNet,LINEARSVM,LINEARRBFM,DecisionTree1,DecisionTree,RANDOMFOREST1,RANDOMFOREST,XGBOOST1,XGBOOST],ignore_index=True)
result

# prompt: dump only knn in the pickle file

import pickle

# Assuming 'knn' is your trained KNeighborsClassifier model
# Replace this with your actual knn model

# Example:
# knn = KNeighborsClassifier(n_neighbors=5) # Replace with your actual knn model
# knn.fit(x_train, y_train) # Replace with your actual training data

with open('knn_model.pkl', 'wb') as f:
  pickle.dump(knn, f)

"""PICKLE CODE FOR ALL ALGORITHMS"""



# # prompt: dump all algorithms in single pickle file code

# import pickle

# # Assuming knn, naivebayes, L1, L2, ElasticNet, LINEARSVM, LINEARRBFM, DecisionTree, RANDOMFOREST, XGBOOST are defined
# # from your previous code.  Replace these with your actual trained model objects.

# # Example: Assuming 'knn' is your KNeighborsClassifier model object
# # knn = KNeighborsClassifier()  # Replace with your actual trained model

# # Create a dictionary to store your models
# models = {
#     'knn': knn,
#     'naivebayes': naivebayes,
#     'L1': L1,
#     'L2': L2,
#     'ElasticNet': ElasticNet,
#     'LINEARSVM': LINEARSVM,
#     'LINEARRBFM': LINEARRBFM,
#     'DecisionTree': DecisionTree,
#     'RANDOMFOREST': RANDOMFOREST,
#     'XGBOOST': XGBOOST
# }


# # Save the dictionary to a pickle file
# with open('all_models.pkl1', 'wb') as file:
#     pickle.dump(models, file)

